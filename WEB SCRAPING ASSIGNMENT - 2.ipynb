{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING ASSIGNMENT - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException,ElementNotVisibleException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0              Data Scientist/Data Analyst-immediate   \n",
      "1  Immediate openings For MIS executive /Data ana...   \n",
      "2                                       Data Analyst   \n",
      "3                                       Data Analyst   \n",
      "4                                       Data Analyst   \n",
      "5              ID&A - Data Analyst - Informatica MDM   \n",
      "6                                       Data Analyst   \n",
      "7                                       Data Analyst   \n",
      "8                                       Data Analyst   \n",
      "9                     Data Analyst - O2C - Bangalore   \n",
      "\n",
      "                                             Company Experience  \\\n",
      "0  CAIA-Center For Artificial Intelligence & Adva...    0-3 Yrs   \n",
      "1                             RANDSTAD INDIA PVT LTD    3-8 Yrs   \n",
      "2                                  Applied Materials   6-11 Yrs   \n",
      "3       Cognizant Technology Solutions India Pvt Ltd    2-3 Yrs   \n",
      "4                      St. John’s Research Institute    0-4 Yrs   \n",
      "5                Shell India Markets Private Limited    6-9 Yrs   \n",
      "6            GlaxoSmithKline Pharmaceuticals Limited    2-7 Yrs   \n",
      "7       Cognizant Technology Solutions India Pvt Ltd    3-4 Yrs   \n",
      "8                Shell India Markets Private Limited    5-8 Yrs   \n",
      "9                             RANDSTAD INDIA PVT LTD    2-4 Yrs   \n",
      "\n",
      "                              Location  \n",
      "0  Chennai, Pune, Bengaluru, Hyderabad  \n",
      "1                            Bengaluru  \n",
      "2                            Bengaluru  \n",
      "3         Mumbai, Bengaluru, Hyderabad  \n",
      "4                            Bengaluru  \n",
      "5                            Bengaluru  \n",
      "6                            Bengaluru  \n",
      "7                   Bengaluru, Kolkata  \n",
      "8                            Bengaluru  \n",
      "9                            Bengaluru  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def data_ana(url):\n",
    "    job_title, company, location, experience = [], [], [], []\n",
    "    driver.get(url)\n",
    "    \n",
    "#Search for job position\n",
    "    job_search = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    job_search.send_keys('Data Analyst')\n",
    "    \n",
    "#Search for location\n",
    "    loc_search = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    loc_search.send_keys('Bangalore')\n",
    "    \n",
    "#Click search button\n",
    "    button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "#loop to scrape required details from each job \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_title.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    driver.quit()\n",
    "#Saving in dataframe\n",
    "    df=pd.DataFrame({'Job Title':job_title[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experience':experience[:10],\n",
    "                     'Location':location[:10]}) \n",
    "    df.to_csv('Data_Analyst.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "data_ana(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0              Data Scientist/Data Analyst-immediate   \n",
      "1  HCL hiring Data scientist with exp in machine ...   \n",
      "2                  Data Scientist - Machine Learning   \n",
      "3                  Data Scientist - Machine Learning   \n",
      "4                  Data Scientist - Machine Learning   \n",
      "5                   Data Scientist -Machine Learning   \n",
      "6  Data Scientist  -  Machine Learning -  Remote ...   \n",
      "7         Lead Data Scientist - Complete Remote Work   \n",
      "8                                     Data Scientist   \n",
      "9  Data Scientist (Healthcare/Pharma Domain prefe...   \n",
      "\n",
      "                                             Company  \\\n",
      "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
      "1                           HCL Technologies Limited   \n",
      "2                                        AugmatrixGo   \n",
      "3                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
      "4                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
      "5                  BLUE YONDER INDIA PRIVATE LIMITED   \n",
      "6                                           Doji Ltd   \n",
      "7                  Techolution India Private Limited   \n",
      "8                      GENPACT India Private Limited   \n",
      "9                      GENPACT India Private Limited   \n",
      "\n",
      "                                         Description  \\\n",
      "0  Job description\\nDear Candidate\\n\\nSchedule a ...   \n",
      "1  Job description\\nDear Candidate,\\n\\nGreetings ...   \n",
      "2  Job description\\nRoles and Responsibilities\\n\\...   \n",
      "3                                                 NA   \n",
      "4                                                 NA   \n",
      "5                                                 NA   \n",
      "6  Job description\\nPlease note that this role wi...   \n",
      "7  Job description\\n\\nWe are looking for qualifie...   \n",
      "8                                                 NA   \n",
      "9                                                 NA   \n",
      "\n",
      "                                            Location  \n",
      "0                Chennai, Pune, Bengaluru, Hyderabad  \n",
      "1                                          Bengaluru  \n",
      "2                                          Bengaluru  \n",
      "3                                          Bengaluru  \n",
      "4                                          Bengaluru  \n",
      "5                                          Bengaluru  \n",
      "6            Delhi NCR, Bengaluru, Anywhere in India  \n",
      "7  Chennai, Pune, Delhi NCR, Mumbai, Bengaluru, H...  \n",
      "8                                          Bengaluru  \n",
      "9                                          Bengaluru  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def data_sci(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    job_title, company, location, description, urls = [], [], [], [],[]\n",
    "\n",
    "#Search for job position\n",
    "    job_search = driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\")\n",
    "    job_search.send_keys('Data Scientist')\n",
    "    \n",
    "#Search for location\n",
    "    loc_search = driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "    loc_search.send_keys('Bangalore')\n",
    "    \n",
    "#Click search button\n",
    "    button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "    button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "#loop to scrape required details from each job     \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_title.append(j.text)\n",
    "                    urls.append(j.get('href'))\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 3: location.append(k.text)\n",
    "            else: pass\n",
    "                \n",
    "    \n",
    "    url_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "    for j in url_tags[:10]:\n",
    "        urls.append(j.get_attribute('href'))\n",
    "    for j in urls:\n",
    "        driver.get(j)\n",
    "        try:\n",
    "            description.append((driver.find_element_by_xpath(\"//section[@class='job-desc']\")).text)\n",
    "        except NoSuchElementException:\n",
    "            description.append(\"NA\")\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "#Saving in dataframe\n",
    "    df=pd.DataFrame({'Job Title':job_title[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Description':description[:10],\n",
    "                     'Location':location[:10]})\n",
    "    df.to_csv('Data_Scientist.csv', index = False)\n",
    "    print(df)\n",
    "\n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.naukri.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Scrape data using the filters available on the webpage for the “Data Scientist” designation. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.The location filter to be used is “Delhi/NCR”.The salary filter to be used is “3-6” lakhs from \"https://www.naukri.com/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Job Title  \\\n",
      "0           Data Scientist - Python/Machine Learning   \n",
      "1     Tech Mahindra hiring For Data Scientist- Noida   \n",
      "2  Data Scientist - Commercial Planning and Analysis   \n",
      "3  Data Scientist - Machine Learning/ Artificial ...   \n",
      "4  GCP Skilled Analytics Resources (Data engineer...   \n",
      "5                                     Data Scientist   \n",
      "6                    Data Scientist Machine Learning   \n",
      "7                                     Data Scientist   \n",
      "8                  Business Analyst - Data Scientist   \n",
      "9                           Analyst - Data Scientist   \n",
      "\n",
      "                                 Company Experience  \\\n",
      "0                                  Jubna    5-8 Yrs   \n",
      "1                      tech mahindra ltd   5-10 Yrs   \n",
      "2                 Air Asia India Limited    1-6 Yrs   \n",
      "3           Talent Acceleration Corridor   6-11 Yrs   \n",
      "4     Aerial Telecom Solutions Pvt. Ltd.    3-8 Yrs   \n",
      "5                 IBM India Pvt. Limited    3-5 Yrs   \n",
      "6                              Delhivery    1-3 Yrs   \n",
      "7  Eighteen Pixels India Private Limited    2-6 Yrs   \n",
      "8            HyreFox Consultants Pvt Ltd    3-5 Yrs   \n",
      "9            HyreFox Consultants Pvt Ltd    1-3 Yrs   \n",
      "\n",
      "                     Location  \n",
      "0                       Noida  \n",
      "1                       Noida  \n",
      "2          Delhi NCR, Gurgaon  \n",
      "3  Delhi/NCR Delhi NCR, Noida  \n",
      "4    Pune, Bengaluru, Gurgaon  \n",
      "5            Gurgaon Gurugram  \n",
      "6                     Gurgaon  \n",
      "7                   Delhi NCR  \n",
      "8                     Gurgaon  \n",
      "9                     Gurgaon  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def data_Sci(url):\n",
    "    job_title, company, location, experience = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for the Data Scientist Designation\n",
    "    driver.find_element_by_xpath(\"//input[@id='qsb-keyword-sugg']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "#Click search button\n",
    "    driver.find_element_by_xpath(\"//div[@class='search-btn']/button\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "#Applying location filter     \n",
    "    job_loc = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=job_loc)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "#Applying Salary Filter    \n",
    "    job_sal = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/i')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=job_sal) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "    \n",
    "#loop to scrape required details from each job   \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'class':'info fleft'})\n",
    "    for x, i in enumerate(jobs):\n",
    "        for y, j in enumerate(i.find_all('a')):\n",
    "            if \"Reviews\" in str(j.text): pass\n",
    "            else:\n",
    "                if y+1 == 1: \n",
    "                    job_title.append(j.text)\n",
    "                else: company.append(j.text)\n",
    "        for z, k in enumerate(i.find('ul', attrs = {\"class\": \"mt-7\"}).find_all(\"li\")):\n",
    "            if z+1 == 2: pass\n",
    "            else: \n",
    "                if z+1 == 1: experience.append(k.text)\n",
    "                else: location.append(k.text)\n",
    "    driver.quit()\n",
    "                    \n",
    "#Saving in dataframe\n",
    "    df=pd.DataFrame({'Job Title':job_title[:10],\n",
    "                     'Company':company[:10],\n",
    "                     'Experience':experience[:10],\n",
    "                     'Location':location[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('DataScientist.csv', index = False)\n",
    "    \n",
    "    \n",
    "# Calling Function\n",
    "data_Sci(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Company name Job post duration Rating\n",
      "0                              Oracle                3d    3.7\n",
      "1  Shell Business operations, Chennai                2d    4.1\n",
      "2                              PayPal                1d      4\n",
      "3  Builder.ai - What would you Build?               25d    3.6\n",
      "4                             Nutanix                8d    3.9\n",
      "5                  Ericsson-Worldwide               16d      4\n",
      "6                           Gainsight               20d    4.4\n",
      "7                             Appirio               15d    4.1\n",
      "8                 Integral Ad Science               18d    3.5\n",
      "9                               Bayer                3d    4.1\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Data_sci(url):\n",
    "    company, days_posted, rating = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Sign-In     \n",
    "    sign_in = driver.find_element_by_xpath('/html/body/div[2]/div/div/div/div/div[1]/article/header/nav/div/div/div[4]/div[1]/a')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=sign_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "\n",
    "#Enter Email address\n",
    "    email = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[1]/div/div/input\")\n",
    "    email.send_keys('priyalagarwal2797@gmail.com')\n",
    "    time.sleep(3)\n",
    "    \n",
    "#Enter password    \n",
    "    passowrd = driver.find_element_by_xpath(\"/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[2]/div/div/input\")\n",
    "    passowrd.send_keys('qwerty1234')\n",
    "    time.sleep(3)\n",
    " \n",
    "    sign_in = driver.find_element_by_xpath('/html/body/div[8]/div/div/div[2]/div[2]/div[2]/div/div/div/div[3]/form/div[3]/div[1]/button')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=sign_in)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for Data Scientist\n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.keyword']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "\n",
    "#Clear the location field\n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").clear()\n",
    "    time.sleep(3)\n",
    "    \n",
    "#Search for location    \n",
    "    driver.find_element_by_xpath(\"//input[@id='sc.location']\").send_keys('Noida')\n",
    "    time.sleep(3)\n",
    "    \n",
    "#Click search button\n",
    "    driver.find_element_by_xpath(\"/html/body/header/nav[1]/div/div/div/div[4]/div[3]/form/div/button/span\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "#Scrape required details    \n",
    "    jobs = soup.find_all('div', attrs ={'class':'jobHeader d-flex justify-content-between align-items-start'})\n",
    "    for i in jobs:\n",
    "        company.append(i.text)\n",
    "        \n",
    "    jobs1 = soup.find_all('div', attrs ={'class':'d-flex align-items-end pl-std css-mi55ob'})\n",
    "    for j in jobs1:\n",
    "        days_posted.append(j.text)\n",
    "        \n",
    "    jobs2 = soup.find_all('div', attrs ={'class':'d-flex flex-column css-fbt9gv e1rrn5ka2'})\n",
    "    for k in jobs2:\n",
    "        rating.append(k.text)\n",
    "        \n",
    "#Saving in dataframe            \n",
    "    df=pd.DataFrame({'Company name':company[:10],\n",
    "                     'Job post duration':days_posted[:10],\n",
    "                     'Rating':rating[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_Data_scientist.csv', index = False)\n",
    "\n",
    "# Calling Function\n",
    "try:\n",
    "    data_sci(\"https://www.glassdoor.co.in/index.htm\")\n",
    "    driver.quit()\n",
    "except NoSuchElementException as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary from https://www.glassdoor.co.in/Salaries/index.htm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Company name Salary count Average Salary Minimum Salary  \\\n",
      "0                       Delhivery           14      12,81,419           456K   \n",
      "1              Ericsson-Worldwide           22       7,52,052           420K   \n",
      "2                       Accenture           37       9,98,925           585K   \n",
      "3       Tata Consultancy Services           83       6,02,000           336K   \n",
      "4                             IBM           73       7,71,657           595K   \n",
      "5              UnitedHealth Group           15      12,22,902           727K   \n",
      "6              Valiance Solutions           10       7,91,015           509K   \n",
      "7                      Innovaccer            9      12,15,138           629K   \n",
      "8  Cognizant Technology Solutions           44      10,21,889           804K   \n",
      "9                   ZS Associates           16      10,00,000           205K   \n",
      "\n",
      "  Maximum Salary  \n",
      "0        11,789K  \n",
      "1         1,636K  \n",
      "2         2,200K  \n",
      "3         1,024K  \n",
      "4         2,769K  \n",
      "5         1,597K  \n",
      "6         1,168K  \n",
      "7         1,719K  \n",
      "8         1,281K  \n",
      "9         1,835K  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Data_Sci(url):\n",
    "    company, avg_sal, min_sal, max_sal, sal_count  = [], [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "#Search for Data Scientist position          \n",
    "    driver.find_element_by_xpath(\"//input[@id='KeywordSearch']\").send_keys('Data Scientist')\n",
    "    time.sleep(3)\n",
    "    \n",
    "#Clear the location field\n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").clear()\n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for Noida location\n",
    "    driver.find_element_by_xpath(\"//input[@id='LocationSearch']\").send_keys('noida')\n",
    "    time.sleep(3)\n",
    "\n",
    "#Click for search button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[3]/div/div[1]/div[1]/div/div/form/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'data-test':'salary-list-items'})\n",
    "\n",
    "    company = [str(i.text).split(\"₹\")[0].split(\" Scientist\")[1].replace(\"- Monthly Intern\",\"\") for i in jobs]\n",
    "    avg_sal = [str(i.text).split()[-1].split(\"₹\")[0].rstrip(\"/yr\") for i in jobs]\n",
    "    min_sal = [str(i.text).split()[-1].split(\"₹\")[1] for i in jobs]\n",
    "    max_sal = [str(i.text).split()[-1].split(\"₹\")[2] for i in jobs]\n",
    "    sal_count = [str(i.text).split(\"salaries\")[1].lstrip(\"See \").rstrip(\" \") for i in jobs]\n",
    "    \n",
    "#Saving in dataframe     \n",
    "    df=pd.DataFrame({'Company name':company[:10],\n",
    "                     'Salary count':sal_count[:10],\n",
    "                     'Average Salary':avg_sal[:10],\n",
    "                     'Minimum Salary':min_sal[:10],\n",
    "                     'Maximum Salary':max_sal[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('glassdoor_Data_scientist_salary.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    Data_Sci(\"https://www.glassdoor.co.in/Salaries/index.htm\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Brand                                Product Description Price  \\\n",
      "0            Rozdeal  Polarized, Night Vision, Riding Glasses, Mirro...   749   \n",
      "1          ROYAL SON         UV Protection Retro Square Sunglasses (88)   599   \n",
      "2     FDA COLLECTION  Gradient, Mirrored, UV Protection Round, Round...   199   \n",
      "3   shah collections  UV Protection, Polarized, Mirrored Rectangular...   219   \n",
      "4         Phenomenal  UV Protection, Mirrored Retro Square Sunglasse...   399   \n",
      "..               ...                                                ...   ...   \n",
      "94            PIRASO              UV Protection Aviator Sunglasses (58)   349   \n",
      "95    FDA COLLECTION  Gradient, Mirrored, UV Protection Round, Round...   188   \n",
      "96    kingsunglasses  Mirrored, UV Protection Wayfarer, Wayfarer, Wa...   284   \n",
      "97        funglasses  UV Protection, Night Vision, Riding Glasses Ro...   170   \n",
      "98            Riffko             UV Protection Wayfarer Sunglasses (55)   235   \n",
      "\n",
      "   Discount  \n",
      "0        81  \n",
      "1        70  \n",
      "2        84  \n",
      "3        78  \n",
      "4        80  \n",
      "..      ...  \n",
      "94       86  \n",
      "95       87  \n",
      "96       81  \n",
      "97       82  \n",
      "98       80  \n",
      "\n",
      "[99 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Flipkart(url):\n",
    "    brand, product, price, discount = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for sunglasses    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sunglasses')\n",
    "    time.sleep(3)\n",
    "\n",
    "#Click for search button    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "#loop to scrape required details till page 3(upto 100)    \n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        Brand = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in Brand:\n",
    "            brand.append(j.text)\n",
    "        Product = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in Product:\n",
    "            product.append(k.text)\n",
    "            \n",
    "        Price = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in Price:\n",
    "            price_tag = str(i.text).split(\"₹\")[1] \n",
    "            price.append(price_tag)\n",
    "            \n",
    "        try:\n",
    "            for i in Price:\n",
    "                discount_tag = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                discount.append(discount_tag)\n",
    "        except NoSuchElementException:\n",
    "            discount.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass \n",
    "#Saving in dataframe    \n",
    "    df=pd.DataFrame({'Brand':brand[:99],\n",
    "                     'Product Description':product[:99],\n",
    "                     'Price':price[:99],\n",
    "                     'Discount':discount[:99]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sunglass.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    Flipkart(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 99 100\n",
      "   Rating      Review Summary  \\\n",
      "0       5    Perfect product!   \n",
      "1       5       Great product   \n",
      "2       5    Perfect product!   \n",
      "3       5  Highly recommended   \n",
      "4       5    Perfect product!   \n",
      "..    ...                 ...   \n",
      "94      3            Terrific   \n",
      "95      5        Does the job   \n",
      "96      3            Terrific   \n",
      "97      4                Nice   \n",
      "98      5           Wonderful   \n",
      "\n",
      "                                               Review  \n",
      "0   Amazing phone with great cameras and better ba...  \n",
      "1   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
      "2   It’s a must buy who is looking for an upgrade ...  \n",
      "3   iphone 11 is a very good phone to buy only if ...  \n",
      "4   Value for money❤️❤️\\nIts awesome mobile phone ...  \n",
      "..                                                ...  \n",
      "94  Have used both iPhone X and iPhone XR and I ca...  \n",
      "95  The phone is awesome undoubtedly and worth the...  \n",
      "96  Switched from Android to Iphone. great experie...  \n",
      "97  Iphone 11 black 64gb is really a cool phone\\n\\...  \n",
      "98  The phone is genuinely very good in terms of p...  \n",
      "\n",
      "[99 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def iphone(url):\n",
    "    rating, summary, review,urls = [], [], [],[]\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "#Loop to fetch urls of each review \n",
    "    url=driver.find_elements_by_xpath(\"//nav[@class='yFHi8N']/a\")\n",
    "    for i in url[0:10]:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "#loop to scrap required review from every page\n",
    "    for url in urls:\n",
    "        driver.get(url)\n",
    "        rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "        for j in rating_tags:\n",
    "            rating.append(j.text)\n",
    "        Summary=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "        for i in Summary:\n",
    "            summary.append(i.text)\n",
    "        for k in driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\"):\n",
    "            review.append(k.text)\n",
    " \n",
    "    driver.close()\n",
    "\n",
    "#Saving in dataframe \n",
    "    print(len(summary),len(rating),len(review))\n",
    "    df=pd.DataFrame({'Rating':rating[:99],\n",
    "                     'Review Summary':summary[:99],\n",
    "                     'Review':review[:99]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_iphone.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "iphone('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace=FLIPKART') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Brand                                Product Description  Price  \\\n",
      "0       Calcados                   Combo Pack Of 3 Sneakers For Men    499   \n",
      "1           Puma               X-Ray 2 Square PACK Sneakers For Men  4,339   \n",
      "2         Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...    499   \n",
      "3   Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men    378   \n",
      "4         Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...    224   \n",
      "..           ...                                                ...    ...   \n",
      "95        Chevit  168 Smart Red Lace-Ups Casuals for Men Sneaker...    224   \n",
      "96      Red Rose                                   Sneakers For Men    359   \n",
      "97      Hotstyle                                   Sneakers For Men    204   \n",
      "98        Chevit  Smart Casuals Canvas Shoes Combo pack of 2 Sne...    299   \n",
      "99     RICHERSON              CAUSAL SHOES FOR MEN Sneakers For Men    479   \n",
      "\n",
      "   Discount  \n",
      "0        66  \n",
      "1        38  \n",
      "2        75  \n",
      "3        62  \n",
      "4        55  \n",
      "..      ...  \n",
      "95       55  \n",
      "96       64  \n",
      "97       59  \n",
      "98       70  \n",
      "99       52  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Sneakers(url):\n",
    "    brand, product, price, discount = [], [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for sneakers\n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\").send_keys('sneakers')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\").click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    driver.find_element_by_xpath(\"/html/body/div/div/div[1]/div[1]/div[2]/div[2]/form/div/button\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "#loop to scrape required details from each mobile till page 3(upto 100)\n",
    "    for g in range(3):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        Brand = soup.find_all('div', attrs ={'class':'_2WkVRV'})\n",
    "        for j in Brand:\n",
    "            brand.append(j.text)\n",
    "        Product = soup.find_all('a', attrs ={'class':'IRpwTa'})\n",
    "        for k in Product:\n",
    "            product.append(k.text)\n",
    "            \n",
    "        Price = soup.find_all('a', attrs ={'class':'_3bPFwb'})\n",
    "        for i in Price:          \n",
    "            price_tag = str(i.text).split(\"₹\")[1] \n",
    "            price.append(price_tag)\n",
    "            \n",
    "        for i in Price:\n",
    "            try:\n",
    "                discount_tag = str(((i.text).split(\"₹\")[2]).split(\"%\")[0])[-2:]\n",
    "                discount.append(discount_tag)\n",
    "            except:\n",
    "                discount.append('-')\n",
    "\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[2]/div[1]/div[2]/div[12]/div/div/nav/a[\"+str(g+10)+\"]\").click()\n",
    "            time.sleep(5)\n",
    "        except NoSuchElementException:\n",
    "            pass \n",
    "#Saving in dataframe    \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Product Description':product[:100],\n",
    "                     'Price':price[:100],\n",
    "                     'Discount':discount[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('flipkart_sneakers.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    Sneakers(\"https://www.flipkart.com/\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Go to the link - https://www.myntra.com/shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Brand                          Description  Price\n",
      "0           Geox           Men Solid Leather Sneakers   6599\n",
      "1           Nike             Men LEGEND REACT 3 Shoes   6199\n",
      "2           Nike               Men Zoom Running Shoes   6399\n",
      "3           Nike         Men ZOOM FLY 3 Running Shoes  11199\n",
      "4      Cole Haan  Men Traveller Leather Penny Loafers   9999\n",
      "..           ...                                  ...    ...\n",
      "95          Geox               Women Slip-On Sneakers   8499\n",
      "96     Cole Haan       Women Solid Leather Ballerinas  10499\n",
      "97          FILA               Women Printed Sneakers   7999\n",
      "98        Clarks           Men Leather Mid-Top Derbys   8999\n",
      "99  UNDER ARMOUR       Women Charged Bandit 5 Running   7999\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Myntra(url):\n",
    "    brand, description, price = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Applying price filter \n",
    "    price_filter = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=price_filter)\n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "\n",
    "#Applying color filter \n",
    "    color_filter = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "    action = ActionChains(driver) \n",
    "    action.click(on_element=color_filter) \n",
    "    action.perform() \n",
    "    time.sleep(3)\n",
    "\n",
    "#loops to scrape required details\n",
    "    for g in range(2):\n",
    "        g = g + 1\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        Brand_tag = soup.find_all('h3', attrs ={'class':'product-brand'})\n",
    "        for i in Brand_tag:\n",
    "            brand.append(i.text)\n",
    "\n",
    "        Description = soup.find_all('h4', attrs ={'class':'product-product'})\n",
    "        for j in Description:\n",
    "            description.append(j.text)\n",
    "\n",
    "        Price_tag = soup.find_all('div', attrs ={'class':'product-price'})\n",
    "        for k in Price_tag:\n",
    "            price.append((k.text).split(\"Rs. \")[1])\n",
    "\n",
    "        driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[3]/a\").click()\n",
    "        time.sleep(5)\n",
    "#Saving in dataframe                          \n",
    "    df=pd.DataFrame({'Brand':brand[:100],\n",
    "                     'Description':description[:100],\n",
    "                     'Price':price[:100]})\n",
    "    print(df)\n",
    "    df.to_csv('myntra_shoes.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    Myntra(\"https://www.myntra.com/shoes\")\n",
    "    driver.quit()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Go to webpage https://www.amazon.in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title Rating  \\\n",
      "0  Dell Inspiron 5501 15.6 Inch FHD Laptop (10th ...    3.4   \n",
      "1  Dell XPS 9570 15.6-inch FHD Laptop (8th Gen Co...    2.5   \n",
      "2  Lenovo IdeaPad Gaming 3i 10th Gen Intel Core i...    3.8   \n",
      "3  HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inc...    3.6   \n",
      "4  Mi Notebook Horizon Edition 14 Intel Core i5-1...    4.2   \n",
      "5  Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...    3.3   \n",
      "6  ASUS ZenBook 14 (2020) Intel Core i7-1165G7 11...    4.3   \n",
      "7  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...    5.0   \n",
      "8  ASUS TUF Gaming F15, 15.6\" FHD 144Hz, Intel Co...    3.8   \n",
      "9  Acer Nitro 5 Intel Core i7 10750H 15.6\" FHD IP...    4.0   \n",
      "\n",
      "                                               Price  \n",
      "0                                             85,890  \n",
      "1                                           1,27,990  \n",
      "2                                             74,990  \n",
      "3                                             74,990  \n",
      "4  54,899\\n\\nSave extra with No Cost EMISave extr...  \n",
      "5                                             96,450  \n",
      "6                                             95,590  \n",
      "7                                           2,68,329  \n",
      "8                                             82,990  \n",
      "9                                             79,990  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "# Function Definition\n",
    "def Amazon(url):\n",
    "    title, rating, price = [], [], []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "#Search for laptops\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\").send_keys('Laptop')\n",
    "    time.sleep(3)\n",
    "\n",
    "#Click search button\n",
    "    driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input\").click()\n",
    "    time.sleep(5)\n",
    "\n",
    "#Applying intel corei7 filter \n",
    "    filter1=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-nostyle a-vertical a-spacing-medium']//li[@id='p_n_feature_thirteen_browse-bin/12598163031']/span/a/div\")\n",
    "    try:\n",
    "        filter1.click()\n",
    "    except ElementNotInteractableException:\n",
    "        filter1.get_attribute('href')\n",
    "#Applying intel corei9 filter\n",
    "    filter2=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-nostyle a-vertical a-spacing-medium']//li[@id='p_n_feature_thirteen_browse-bin/16757432031']/span/a/div\")\n",
    "    try:\n",
    "        filter2.click()\n",
    "    except ElementNotInteractableException:\n",
    "        filter2.get_attribute('href')\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    jobs = soup.find_all('div', attrs ={'data-component-type':'s-search-result'})\n",
    "\n",
    "#loops for fetching required details\n",
    "    for g in jobs:\n",
    "        title_tag = (g.text).split(\"\\n\\n\\n\\n\\n\")[5]\n",
    "        if title_tag == '':\n",
    "            title_tag = (g.text).split(\"\\n\\n\\n\\n\\n\")[8] \n",
    "            title.append(title_tag.lstrip(\"\\n\\n\\n\"))\n",
    "        else:\n",
    "            title.append(title_tag.lstrip(\"\\n\"))\n",
    "                         \n",
    "        try:\n",
    "            Rating_tag = str((g.text).split(\" out of 5 stars\")[0][-3:]).replace(\"\\n\\n\\n\",\"-\") \n",
    "            rating.append(Rating_tag)\n",
    "        except:\n",
    "            rating.append(\"-\")\n",
    "        try:\n",
    "            price_tag = (g.text).split(\"₹\")[1] \n",
    "            price.append(price_tag)\n",
    "        except:\n",
    "            price.append(\"-\")\n",
    "    driver.quit()\n",
    "#Saving in dataframe    \n",
    "    df=pd.DataFrame({'Title':title[:10],\n",
    "                     'Rating':rating[:10],\n",
    "                     'Price':price[:10]})\n",
    "    print(df)\n",
    "    df.to_csv('amazon_laptop.csv', index = False)\n",
    "    \n",
    "# Calling Function\n",
    "try:\n",
    "    Amazon(\"https://www.amazon.in/\")\n",
    "    driver.quit()\n",
    "except NoSuchElementException as e:\n",
    "    print(e)\n",
    "    driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
